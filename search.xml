<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>深度学习框架搭建课程二（反向传播、激活函数、拓扑排序）</title>
      <link href="/2020/08/04/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6%E6%90%AD%E5%BB%BA%E8%AF%BE%E7%A8%8B%E4%BA%8C%EF%BC%88%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E3%80%81%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E3%80%81%E6%8B%93%E6%89%91%E6%8E%92%E5%BA%8F%EF%BC%89/"/>
      <url>/2020/08/04/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6%E6%90%AD%E5%BB%BA%E8%AF%BE%E7%A8%8B%E4%BA%8C%EF%BC%88%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E3%80%81%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E3%80%81%E6%8B%93%E6%89%91%E6%8E%92%E5%BA%8F%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<h1 id="Lesson2-反向传播-激活函数-图传播的拓扑结构"><a href="#Lesson2-反向传播-激活函数-图传播的拓扑结构" class="headerlink" title="Lesson2 反向传播 激活函数 图传播的拓扑结构"></a>Lesson2 反向传播 激活函数 图传播的拓扑结构</h1><h2 id="Review"><a href="#Review" class="headerlink" title="Review"></a>Review</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_boston</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X, y = load_boston()[<span class="string">'data'</span>],load_boston()[<span class="string">'target'</span>]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">room_index = <span class="number">5</span></span><br><span class="line">X_rm = X[:, room_index]</span><br></pre></td></tr></table></figure><h2 id="zip"><a href="#zip" class="headerlink" title="zip()"></a>zip()</h2><ul><li><strong>用于将可迭代的对象作为参数，将对象中对应的元素打包成一个个元组，然后返回由这些元组组成的对象，这样做的好处是节约了不少的内存。</strong></li><li><strong>可以使用 list() 转换来输出列表</strong></li><li><strong>如果各个迭代器的元素个数不一致，则返回列表长度与最短的对象相同，利用 * 号操作符，可以将元组解压为列表</strong></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">###############zip()示例################</span></span><br><span class="line"></span><br><span class="line">a = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]</span><br><span class="line">b = [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]</span><br><span class="line">c = [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>]</span><br><span class="line">zipped = zip(a,b)</span><br><span class="line">print(zipped)</span><br><span class="line">print(list(zipped))</span><br><span class="line">print(list(zip(a,c)))</span><br><span class="line"></span><br><span class="line">a1, a2 = zip(*zip(a,b))</span><br><span class="line">print(a1, a2)</span><br></pre></td></tr></table></figure><pre><code>&lt;zip object at 0x7fd68191da00&gt;[(1, 4), (2, 5), (3, 6)][(1, 4), (2, 5), (3, 6)](1, 2, 3) (4, 5, 6)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss</span><span class="params">(y, yhat)</span>:</span> </span><br><span class="line">    <span class="comment"># 如何定义loss函数，是一个单独的研究方向</span></span><br><span class="line">    <span class="comment"># loss尽量让它是一个凸函数</span></span><br><span class="line">    <span class="comment"># 凸函数上找到最小值或者最大值的情况，我们叫做优化问题optimize</span></span><br><span class="line">    <span class="comment"># Convex Optimization</span></span><br><span class="line">    sum_ = sum([(y_i - yhat_i) ** <span class="number">2</span> <span class="keyword">for</span> y_i, yhat_i <span class="keyword">in</span> zip(y, yhat)])</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> sum_ / len(y)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">partial_k</span><span class="params">(x, y, yhat)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">-2</span> * np.mean((np.array(y)-np.array(yhat))*np.array(x))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">partial_b</span><span class="params">(y, yhat)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">-2</span> * np.mean(np.array(y)-np.array(yhat))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span><span class="params">(x, k, b)</span>:</span> <span class="comment"># 本课将模型简化成了一种线性关系 </span></span><br><span class="line">    <span class="comment"># RNN, CNN, Batch_normalization</span></span><br><span class="line">    <span class="comment"># CNN</span></span><br><span class="line">    <span class="keyword">return</span> k * x + b</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">optimizer</span><span class="params">(k, b, X_rm, y, price_predicted, learning_rate)</span>:</span></span><br><span class="line">    <span class="comment"># Adam, momentum...</span></span><br><span class="line">    k_gradient = partial_k(X_rm, y, price_predicted)</span><br><span class="line">    b_gradient = partial_b(y, price_predicted)</span><br><span class="line"></span><br><span class="line">    k = k + (<span class="number">-1</span> * k_gradient) * learning_rate</span><br><span class="line">    b = b + (<span class="number">-1</span> * b_gradient) * learning_rate</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> k, b</span><br><span class="line"></span><br><span class="line">trying_time = <span class="number">50000</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#Initialization Parameters</span></span><br><span class="line">k = random.random() * <span class="number">100</span> - <span class="number">200</span></span><br><span class="line">b = random.random() * <span class="number">100</span> - <span class="number">200</span></span><br><span class="line">learning_rate = <span class="number">1e-3</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(trying_time):</span><br><span class="line">    price_predicted = model(X_rm, k, b)</span><br><span class="line">    loss_value = loss(y, price_predicted)</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> i % <span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line">        print(<span class="string">'step:&#123;&#125;--k==&#123;&#125;, b==&#123;&#125;, loss=&#123;&#125;'</span>.format(i, k, b, loss_value))</span><br><span class="line">    k, b = optimizer(k, b, X_rm, y, price_predicted, learning_rate)</span><br></pre></td></tr></table></figure><pre><code>step:0--k==-139.97574080889325, b==-189.5565944077231, loss=1202989.4946001442step:1000--k==28.789202260854257, b==-159.9025443960182, loss=236.82742611841562step:2000--k==28.321423579850478, b==-156.92694900374082, loss=227.7541131560305step:3000--k==27.86475963738596, b==-154.02205577495837, loss=219.10685380752045step:4000--k==27.418946339739048, b==-151.1861847783419, loss=210.86564196457158step:5000--k==26.98372586823378, b==-148.41769599886837, loss=203.0114109414113step:6000--k==26.558846530140826, b==-145.7149883893845, loss=195.52598936255154step:7000--k==26.144062613120745, b==-143.07649894470254, loss=188.3920591218916step:8000--k==25.739134243125765, b==-140.5007017976972, loss=181.59311531592934step:9000--k==25.343827245678145, b==-137.98610733688074, loss=175.1134280583828step:10000--k==24.957913010444287, b==-135.5312613449428, loss=168.9380060878701step:11000--k==24.58116835902728, b==-133.1347441577636, loss=163.05256208446227step:12000--k==24.213375415900217, b==-130.7951698434049, loss=157.44347961485315step:13000--k==23.854321482406256, b==-128.51118540061006, loss=152.09778162967729step:14000--k==23.50379891375279, b==-126.28146997634929, loss=147.00310044009777step:15000--k==23.161604998928414, b==-124.10473410195712, loss=142.14764910419603step:16000--k==22.827541843472655, b==-121.97971894741703, loss=137.52019415696014step:17000--k==22.501416255031483, b==-119.90519559336649, loss=133.11002962078763step:18000--k==22.183039631632305, b==-117.87996432040093, loss=128.90695223637664step:19000--k==21.872227852613182, b==-115.90285391526179, loss=124.90123785668611step:20000--k==21.56880117214389, b==-113.97272099351123, loss=121.08361894936512step:21000--k==21.272584115277002, b==-112.08844933830095, loss=117.44526315559573step:22000--k==20.983405376468696, b==-110.24894925485091, loss=113.97775285574032step:23000--k==20.701097720511058, b==-108.4531569402683, loss=110.67306569452485step:24000--k==20.425497885818103, b==-106.70003386833821, loss=107.52355602069466step:25000--k==20.156446490009778, b==-104.9885661889324, loss=104.52193719820313step:26000--k==19.893787937739386, b==-103.31776414168878, loss=101.66126474801261step:27000--k==19.63737033071113, b==-101.68666148362233, loss=98.93492028149987step:28000--k==19.387045379835737, b==-100.09431493033665, loss=96.33659618829992step:29000--k==19.142668319473103, b==-98.53980361051134, loss=93.86028104315582step:30000--k==18.904097823713013, b==-97.02222853335356, loss=91.50024569802068step:31000--k==18.67119592464453, b==-95.54071206870042, loss=89.25103002722422step:32000--k==18.443827932567885, b==-94.09439743947698, loss=87.10743029504896step:33000--k==18.221862358101898, b==-92.68244822621246, loss=85.0644871164814step:34000--k==18.00517083614213, b==-91.30404788332926, loss=83.11747398328737step:35000--k==17.79362805162611, b==-89.95839926692656, loss=81.26188632886817step:36000--k==17.587111667062057, b==-88.64472417378221, loss=79.49343110659359step:37000--k==17.385502251780128, b==-87.36226289131133, loss=77.8080168575062step:38000--k==17.18868321286402, b==-86.11027375821422, loss=76.20174424440974step:39000--k==16.996540727724163, b==-84.88803273556643, loss=74.67089703044967step:40000--k==16.808963678272896, b==-83.6948329880994, loss=73.21193348130983step:41000--k==16.625843586663674, b==-82.52998447542974, loss=71.82147817113456step:42000--k==16.447074552556995, b==-81.39281355300069, loss=70.496314173216step:43000--k==16.272553191877268, b==-80.28266258250707, loss=69.23337561738542step:44000--k==16.10217857702466, b==-79.19888955157587, loss=68.02974059688358step:45000--k==15.935852178507512, b==-78.14086770248328, loss=66.88262440830098step:46000--k==15.773477807961688, b==-77.10798516969385, loss=65.78937310895009step:47000--k==15.61496156252368, b==-76.09964462601131, loss=64.7474573767608step:48000--k==15.460211770525754, b==-75.11526293713911, loss=63.754466658498906step:49000--k==15.309138938481153, b==-74.15427082444685, loss=62.80810359276205</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">plt.scatter(X_rm, y)</span><br><span class="line">plt.plot(X_rm, model(X_rm, k, b), color=<span class="string">'red'</span>)</span><br></pre></td></tr></table></figure><pre><code>[&lt;matplotlib.lines.Line2D at 0x7fd68193f290&gt;]</code></pre><p><img src="https://cdn.jsdelivr.net/gh/rgwang/CDN@latest/2020/08/04/d4f5269fd9dd6c4a80e1b763c9eae2ea.png" alt="output_8_1"></p><h2 id="世界中真实的关系大多都不是简单的线性关系"><a href="#世界中真实的关系大多都不是简单的线性关系" class="headerlink" title="世界中真实的关系大多都不是简单的线性关系"></a>世界中真实的关系大多都不是简单的线性关系</h2><h2 id="我们能不能构建一些基本的模块，然后用模块来组合成复杂的函数"><a href="#我们能不能构建一些基本的模块，然后用模块来组合成复杂的函数" class="headerlink" title="我们能不能构建一些基本的模块，然后用模块来组合成复杂的函数?"></a>我们能不能构建一些基本的模块，然后用模块来组合成复杂的函数?</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(x)</span>:</span> <span class="comment"># basic sub-model : Transfer: Activation Function激活函数</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span>/(<span class="number">1</span> + np.exp(-x))</span><br><span class="line"></span><br><span class="line">x = np.linspace(<span class="number">-10</span>, <span class="number">10</span>,<span class="number">1000</span>)</span><br><span class="line">plt.plot(x, sigmoid(x))</span><br></pre></td></tr></table></figure><pre><code>[&lt;matplotlib.lines.Line2D at 0x7fd6810a2a50&gt;]</code></pre><p><img src="https://cdn.jsdelivr.net/gh/rgwang/CDN@latest/2020/08/04/f1d501a38c207b1d18f7f27a57ac3a7d.png" alt="output_10_1"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">random_linear</span><span class="params">(x)</span>:</span></span><br><span class="line">    k, b = np.random.normal(), np.random.normal()</span><br><span class="line">    <span class="keyword">return</span> k * x + b * x</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_test_x</span><span class="params">(n)</span>:</span></span><br><span class="line">    max_, min_ = <span class="number">500</span>, <span class="number">-500</span></span><br><span class="line">    <span class="keyword">return</span> np.random.choice(np.linspace(min_, max_, <span class="number">20000</span>), n)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">    test_x = np.linspace(<span class="number">-200</span>, <span class="number">200</span>, <span class="number">2000</span>)</span><br><span class="line">    plt.plot(random_linear(sigmoid(random_linear(test_x))))</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/rgwang/CDN@latest/2020/08/04/828704e97975df77532808c409fb05ad.png" alt="output_13_0"></p><h2 id="理论上，所有的函数都可以用多层的线性函数-非线性变化来拟合"><a href="#理论上，所有的函数都可以用多层的线性函数-非线性变化来拟合" class="headerlink" title="理论上，所有的函数都可以用多层的线性函数+非线性变化来拟合"></a>理论上，所有的函数都可以用多层的线性函数+非线性变化来拟合</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">relu</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> x * (x &gt; <span class="number">0</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tanh</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> np.tanh(x)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x = np.linspace(<span class="number">-10</span>, <span class="number">10</span>, <span class="number">1000</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plt.plot(x, relu(x))</span><br></pre></td></tr></table></figure><pre><code>[&lt;matplotlib.lines.Line2D at 0x7fd680f39b10&gt;]</code></pre><p><img src="https://cdn.jsdelivr.net/gh/rgwang/CDN@latest/2020/08/04/83b771fa653af1ab06f5c34b5e2ec773.png" alt="output_18_1"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plt.plot(x, tanh(x))</span><br></pre></td></tr></table></figure><pre><code>[&lt;matplotlib.lines.Line2D at 0x7fd680fcb890&gt;]</code></pre><p><img src="https://cdn.jsdelivr.net/gh/rgwang/CDN@latest/2020/08/04/529615a8a6e631888d886345cadf464c.png" alt="output_19_1"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">so_many_layers</span><span class="params">(layers, x)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> len(layers) == <span class="number">1</span>: <span class="keyword">return</span> layers[<span class="number">-1</span>](x)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> so_many_layers(layers[<span class="number">1</span>:], layers[<span class="number">0</span>](x)) <span class="comment"># 递归</span></span><br></pre></td></tr></table></figure><h2 id="北大ACM教练指出算法里最有意义的三种方法"><a href="#北大ACM教练指出算法里最有意义的三种方法" class="headerlink" title="北大ACM教练指出算法里最有意义的三种方法"></a>北大ACM教练指出算法里最有意义的三种方法</h2><ol><li>随机模拟 Randomization</li><li>递归 Recursion</li><li>动态规划 Dynamic Programming</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">layers = [random_linear, relu, random_linear, tanh, random_linear, sigmoid]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">20</span>):</span><br><span class="line">    plt.plot(so_many_layers(layers, x))</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/rgwang/CDN@latest/2020/08/04/4d8cbab7c469d6fbe94ff3410cfacdfc.png" alt="output_23_0"></p><ul><li>神经网络可理解成乐高积木</li><li>有一群人（最重要的一群人）发明新的乐高积木模块</li><li>还有一群人研究积木特别高的时候怎样不倒塌</li><li>还有人研究积木怎样按照我们的需求自己组合</li><li>还有一群人研究搭建好的积木怎么用或者如何用现在的积木模块构造想模仿的问题</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> functools <span class="keyword">import</span> reduce</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">apply</span><span class="params">(func1, func2)</span>:</span> <span class="keyword">return</span> <span class="keyword">lambda</span> x:func2(func1(x))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">20</span>):</span><br><span class="line">    plt.plot(reduce(apply, layers)(x))</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/rgwang/CDN@latest/2020/08/04/6a5a5bb5a692650d7503742c3f7d094c.png" alt="output_27_0"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">price</span><span class="params">(x, k, b)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> k * x + b</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear</span><span class="params">(x, k1, b1)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> k1 * x + b1</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_partial</span><span class="params">(k, b, x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> k</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span>/(<span class="number">1</span> + np.exp(-x))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid_partial</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> sigmoid(x) * (<span class="number">1</span> - sigmoid(x)) </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span><span class="params">(x, k1, k2, b1, b2)</span>:</span></span><br><span class="line">    output1 = linear(x, k1, b1)</span><br><span class="line">    output2 = sigmoid(output1)</span><br><span class="line">    output3 = linear(x, k2, b2)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> output3</span><br><span class="line"></span><br><span class="line">trying_time = <span class="number">50000</span></span><br></pre></td></tr></table></figure><script type="math/tex; mode=display">\sigma (x)=\frac{1}{1+e^{-x}}</script><script type="math/tex; mode=display">loss(y,\hat y)=\frac{1}{n}\sum (y-\hat y)^2</script><script type="math/tex; mode=display">\hat y=k_2\sigma(g)+b_2</script><script type="math/tex; mode=display">g=k_1x+b_1</script><script type="math/tex; mode=display">\frac{\partial loss}{\partial k_1}=\frac{\partial loss}{\partial \hat y}\frac{\partial \hat y}{\partial \sigma}\frac{\partial \sigma}{\partial g}\frac{\partial g}{\partial k_1}</script><ul><li>理论上，线性+非线性的组合可以拟合任意函数</li><li>为什么还要提出如CNN,RNN,Transformer等网络？</li><li>因为拟合函数时，每增加一个参数维度，所需数据量大约须增加10倍</li><li>为了减少需要的数据量，提出了上述特殊的网络，利用权值共享减少参数量，从而减少数据量</li></ul><h1 id="计算图computing-graph"><a href="#计算图computing-graph" class="headerlink" title="计算图computing graph"></a>计算图computing graph</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">computing_graph = &#123;</span><br><span class="line">    <span class="string">'x1'</span>:[<span class="string">'linear-01'</span>],</span><br><span class="line">    <span class="string">'k1'</span>:[<span class="string">'linear-01'</span>],</span><br><span class="line">    <span class="string">'b1'</span>:[<span class="string">'linear-01'</span>],</span><br><span class="line">    <span class="string">'linear-01'</span>:[<span class="string">'sigmoid'</span>],</span><br><span class="line">    <span class="string">'sigmoid'</span>:[<span class="string">'linear-02'</span>],</span><br><span class="line">    <span class="string">'k2'</span>:[<span class="string">'linear-02'</span>],</span><br><span class="line">    <span class="string">'b2'</span>:[<span class="string">'linear-02'</span>],</span><br><span class="line">    <span class="string">'linear-02'</span>:[<span class="string">'loss'</span>]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> networkx <span class="keyword">as</span> nx <span class="comment"># 画神经网络流程图</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">graph = nx.DiGraph(computing_graph)</span><br><span class="line">layout = nx.layout.spring_layout(graph)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nx.draw(graph, layout, with_labels=<span class="literal">True</span>, node_color=<span class="string">'red'</span>)</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/rgwang/CDN@latest/2020/08/04/f49f8133d98e7810a2e2018495372ff6.png" alt=""></p><h1 id="拓扑排序"><a href="#拓扑排序" class="headerlink" title="拓扑排序"></a>拓扑排序</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">visited_procedure</span><span class="params">(graph, position, visited_order, step, sub_plot_index=None, colors=<span class="params">(<span class="string">'red'</span>, <span class="string">'green'</span>)</span>)</span>:</span></span><br><span class="line">    <span class="comment"># 将图graph按照访问顺序visited_order变换颜色</span></span><br><span class="line">    changed = visited_order[:step] <span class="keyword">if</span> step <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> visited_order <span class="comment"># 用step设置被访问的节点</span></span><br><span class="line">    </span><br><span class="line">    before, after = colors <span class="comment"># before代表未被访问的节点颜色，after代表已被访问的节点颜色</span></span><br><span class="line">    </span><br><span class="line">    color_map = [after <span class="keyword">if</span> c <span class="keyword">in</span> changed <span class="keyword">else</span> before <span class="keyword">for</span> c <span class="keyword">in</span> graph] <span class="comment"># 若节点c属于被访问的节点，则颜色设置为after;否则颜色设置为before</span></span><br><span class="line">    </span><br><span class="line">    nx.draw(graph, position, node_color=color_map, with_labels=<span class="literal">True</span>,ax=sub_plot_index) <span class="comment"># 画图，ax=sub_plot_index设置该幅图所在子图的索引</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">visited_order = [<span class="string">'x1'</span>,<span class="string">'b1'</span>,<span class="string">'k1'</span>,<span class="string">'linear-01'</span>,<span class="string">'sigmoid'</span>,<span class="string">'b2'</span>,<span class="string">'k2'</span>,<span class="string">'linear-02'</span>,<span class="string">'loss'</span>]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">visited_procedure(graph, layout, visited_order, step = <span class="literal">None</span>)</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/rgwang/CDN@latest/2020/08/04/8e90b1bb66857ac3b987784e22b48894.png" alt="output_39_0"></p><h2 id="Forward-Propagation"><a href="#Forward-Propagation" class="headerlink" title="Forward Propagation"></a>Forward Propagation</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">dimension = int(len(visited_order)**<span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line">fig, ax = plt.subplots(dimension, dimension+<span class="number">1</span>,figsize=(<span class="number">15</span>,<span class="number">15</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(visited_order)+<span class="number">1</span>):</span><br><span class="line">    ix = np.unravel_index(i, ax.shape) <span class="comment"># 返回索引i在形为ax.shape的数组里的位置</span></span><br><span class="line">    print(ix)</span><br><span class="line">    plt.sca(ax[ix]) <span class="comment"># plt.sca(ax[index])选择显示哪个图</span></span><br><span class="line">    ax[ix].title.set_text(<span class="string">'Forward Propagation Step:&#123;&#125;'</span>.format(i))</span><br><span class="line">    visited_procedure(graph, layout, visited_order, step=i, sub_plot_index=ax[ix])</span><br></pre></td></tr></table></figure><pre><code>(0, 0)(0, 1)(0, 2)(0, 3)(1, 0)(1, 1)(1, 2)(1, 3)(2, 0)(2, 1)</code></pre><p><img src="https://cdn.jsdelivr.net/gh/rgwang/CDN@latest/2020/08/04/ea5d08641ff04992745995ab966cb296.png" alt="output_41_1"></p><h2 id="Backward-Propagation"><a href="#Backward-Propagation" class="headerlink" title="Backward Propagation"></a>Backward Propagation</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">dimension = int(len(visited_order)**<span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line">fig, ax = plt.subplots(dimension, dimension+<span class="number">1</span>,figsize=(<span class="number">15</span>,<span class="number">15</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(visited_order)+<span class="number">1</span>):</span><br><span class="line">    ix = np.unravel_index(i, ax.shape) <span class="comment"># 返回索引i在形为ax.shape的数组里的位置</span></span><br><span class="line">    plt.sca(ax[ix]) <span class="comment"># plt.sca(ax[index])选择显示哪个图</span></span><br><span class="line">    ax[ix].title.set_text(<span class="string">'Forward Propagation Step:&#123;&#125;'</span>.format(i))</span><br><span class="line">    visited_procedure(graph, layout, visited_order[::<span class="number">-1</span>], step=i, sub_plot_index=ax[ix], colors=(<span class="string">'green'</span>,<span class="string">'red'</span>))</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/rgwang/CDN@latest/2020/08/04/5bf9fad724fbcccd68410ca084ffe23a.png" alt="output_43_0"></p><h2 id="拓扑算法"><a href="#拓扑算法" class="headerlink" title="拓扑算法"></a>拓扑算法</h2><ol><li>找到一个只有输出，没有输入的节点（若有多个就随机选择其中一个）</li><li>删除并记录该节点</li><li>把该节点的输出连接删除</li><li>重复上述步骤，直到将所有节点全部删除</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">toplogic</span><span class="params">(graph)</span>:</span> <span class="comment"># 拓扑排序函数，返回根据graph而得到的节点顺序列表</span></span><br><span class="line">    sorted_node = []</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">while</span> len(graph) &gt; <span class="number">0</span>:</span><br><span class="line">        all_inputs = []</span><br><span class="line">        all_outputs = []</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> n <span class="keyword">in</span> graph:</span><br><span class="line">            all_inputs += graph[n] <span class="comment"># 收集所有有输入的节点</span></span><br><span class="line">            all_outputs.append(n) <span class="comment"># 收集所有有输出的节点</span></span><br><span class="line">            </span><br><span class="line">        all_inputs = set(all_inputs)</span><br><span class="line">        all_outputs = set(all_outputs)</span><br><span class="line">        <span class="comment">#print(all_inputs)</span></span><br><span class="line">        <span class="comment">#print(all_outputs)</span></span><br><span class="line">        need_remove = all_outputs - all_inputs </span><br><span class="line">        <span class="comment">#print(need_remove)</span></span><br><span class="line">        <span class="keyword">if</span> len(need_remove) &gt; <span class="number">0</span>:</span><br><span class="line">            node = random.choice(list(need_remove))</span><br><span class="line">            <span class="keyword">if</span> len(graph) == <span class="number">1</span>: temp = graph[node]</span><br><span class="line">            graph.pop(node) <span class="comment"># 删除该节点</span></span><br><span class="line">            sorted_node.append(node)</span><br><span class="line">            <span class="keyword">if</span> len(graph) &lt; <span class="number">1</span>: sorted_node += temp</span><br><span class="line"></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> sorted_node</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">computing_graph = &#123; <span class="comment"># 人为设计一种网络结构computing_graph，key-value代表一段网络的输入和输出</span></span><br><span class="line">    <span class="string">'x1'</span>:[<span class="string">'linear-01'</span>],</span><br><span class="line">    <span class="string">'k1'</span>:[<span class="string">'linear-01'</span>],</span><br><span class="line">    <span class="string">'b1'</span>:[<span class="string">'linear-01'</span>],</span><br><span class="line">    <span class="string">'linear-01'</span>:[<span class="string">'sigmoid'</span>],</span><br><span class="line">    <span class="string">'sigmoid'</span>:[<span class="string">'linear-02'</span>],</span><br><span class="line">    <span class="string">'k2'</span>:[<span class="string">'linear-02'</span>],</span><br><span class="line">    <span class="string">'b2'</span>:[<span class="string">'linear-02'</span>],</span><br><span class="line">    <span class="string">'linear-02'</span>:[<span class="string">'loss'</span>]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">visited_order_by_algorithm = toplogic(computing_graph) <span class="comment"># 通过toplogic函数返回computing_graph网络在传播时的先后顺序</span></span><br><span class="line"></span><br><span class="line">dimension = int(len(visited_order_by_algorithm)**<span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line">fig, ax = plt.subplots(dimension, dimension+<span class="number">1</span>,figsize=(<span class="number">15</span>,<span class="number">15</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(visited_order_by_algorithm)+<span class="number">1</span>):</span><br><span class="line">    ix = np.unravel_index(i, ax.shape) <span class="comment"># 返回索引i在形为ax.shape的数组里的位置</span></span><br><span class="line">    plt.sca(ax[ix]) <span class="comment"># plt.sca(ax[index])选择显示哪个图</span></span><br><span class="line">    ax[ix].title.set_text(<span class="string">'Forward Propagation Step:&#123;&#125;'</span>.format(i))</span><br><span class="line">    visited_procedure(graph, layout, visited_order_by_algorithm, step=i, sub_plot_index=ax[ix])</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/rgwang/CDN@latest/2020/08/04/432bb1ed720dd7c4745d2a2130cec21c.png" alt="output_47_0"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">dimension = int(len(visited_order_by_algorithm)**<span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line">fig, ax = plt.subplots(dimension, dimension+<span class="number">1</span>,figsize=(<span class="number">15</span>,<span class="number">15</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(visited_order_by_algorithm)+<span class="number">1</span>):</span><br><span class="line">    ix = np.unravel_index(i, ax.shape) <span class="comment"># 返回索引i在形为ax.shape的数组里的位置</span></span><br><span class="line">    plt.sca(ax[ix]) <span class="comment"># plt.sca(ax[index])选择显示哪个图</span></span><br><span class="line">    ax[ix].title.set_text(<span class="string">'Forward Propagation Step:&#123;&#125;'</span>.format(i))</span><br><span class="line">    visited_procedure(graph, layout, visited_order_by_algorithm[::<span class="number">-1</span>], step=i, sub_plot_index=ax[ix], colors=(<span class="string">'green'</span>,<span class="string">'red'</span>))</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/rgwang/CDN@latest/2020/08/04/20050fc88eea927c46099725ce72370e.png" alt="output_48_0"></p>]]></content>
      
      
      <categories>
          
          <category> Deep Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DL </tag>
            
            <tag> Backward Propagation </tag>
            
            <tag> 拓扑排序 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度学习框架搭建课程一（线性回归与梯度下降）</title>
      <link href="/2020/08/03/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6%E6%90%AD%E5%BB%BA%E8%AF%BE%E7%A8%8B%E4%B8%80%EF%BC%88%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E4%B8%8E%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%EF%BC%89/"/>
      <url>/2020/08/03/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6%E6%90%AD%E5%BB%BA%E8%AF%BE%E7%A8%8B%E4%B8%80%EF%BC%88%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E4%B8%8E%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<ul><li>回归(Regression)-&gt;拟合、预测</li><li>分类(Classification)-&gt;输出类别</li></ul><h2 id="根据卧室面积预测房价"><a href="#根据卧室面积预测房价" class="headerlink" title="根据卧室面积预测房价"></a>根据卧室面积预测房价</h2><h3 id="加载数据"><a href="#加载数据" class="headerlink" title="加载数据"></a>加载数据</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_boston <span class="comment"># 从sklearn库中调用波士顿房价数据</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data = load_boston()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X, y = data[<span class="string">'data'</span>], data[<span class="string">'target'</span>]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X_rm = X[:, <span class="number">5</span>] <span class="comment"># 取出X中房间面积的数据</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plt.scatter(X_rm, y)</span><br></pre></td></tr></table></figure><pre><code>&lt;matplotlib.collections.PathCollection at 0x7fc0122d41d0&gt;</code></pre><p><img src="https://cdn.jsdelivr.net/gh/rgwang/CDN@latest/2020/08/04/cfdeaa2eba6c0d289ed1b81f3cd0a003.png" alt=""></p><h2 id="本课为线性回归，故需找出一条最佳的直线-y-wx-b-，来拟合卧室和房价的关系"><a href="#本课为线性回归，故需找出一条最佳的直线-y-wx-b-，来拟合卧室和房价的关系" class="headerlink" title="本课为线性回归，故需找出一条最佳的直线$y=wx+b$，来拟合卧室和房价的关系"></a>本课为线性回归，故需找出一条最佳的直线$y=wx+b$，来拟合卧室和房价的关系</h2><h3 id="第一种方法：随机找，记录最优值。"><a href="#第一种方法：随机找，记录最优值。" class="headerlink" title="第一种方法：随机找，记录最优值。"></a>第一种方法：随机找，记录最优值。</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> random.randint(<span class="number">-50</span>,<span class="number">50</span>) * x + random.randint(<span class="number">-50</span>,<span class="number">50</span>)</span><br><span class="line"></span><br><span class="line">plt.scatter(X_rm, y)</span><br><span class="line">plt.plot(X_rm, f(X_rm), color = <span class="string">'red'</span>)</span><br></pre></td></tr></table></figure><pre><code>[&lt;matplotlib.lines.Line2D at 0x7fc010fe5090&gt;]</code></pre><p><img src="https://cdn.jsdelivr.net/gh/rgwang/CDN@latest/2020/08/04/70a629eb6d9968bbe6fe732622265163.png" alt="output_12_1"></p><p><strong>判断拟合结果好与不好的标准(Evaluation)</strong></p><ul><li>存在一组$x$，假设一个函数$f(x)$，输出估计的$\hat y$，衡量输出结果的好坏在于衡量真实$y$与估计$\hat y$之间的差距。</li></ul><script type="math/tex; mode=display">L1\_loss=\frac {1}{n}\sum_{i=1}^n|y_{true_i}-\hat y_i|</script><script type="math/tex; mode=display">L2\_loss=\frac {1}{n}\sum_{i=1}^n(y_{true_i}-\hat y_i)^2</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">l2_loss</span><span class="params">(y,yhat)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> np.mean((np.array(y) - np.array(yhat)) ** <span class="number">2</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">price</span><span class="params">(x, k, b)</span>:</span> <span class="comment"># 线性拟合模型</span></span><br><span class="line">    <span class="keyword">return</span> k * x + b</span><br><span class="line"></span><br><span class="line">trying_time = <span class="number">1000</span></span><br><span class="line">min_loss = float(<span class="string">'inf'</span>)</span><br><span class="line">best_k, best_b = <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">loss_update = []</span><br><span class="line"><span class="comment">#######################随机找拟合模型，记录最优情况#######################</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(trying_time):</span><br><span class="line">    k = random.randint(<span class="number">-200</span>,<span class="number">200</span>)</span><br><span class="line">    b = random.randint(<span class="number">-200</span>,<span class="number">200</span>)</span><br><span class="line">    </span><br><span class="line">    yhat = price(X_rm, k, b)</span><br><span class="line">    </span><br><span class="line">    L2 = l2_loss(y=y, yhat=yhat)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> L2 &lt; min_loss:</span><br><span class="line">        min_loss = L2</span><br><span class="line">        best_k, best_b = k, b</span><br><span class="line">        loss_update.append([i, L2])</span><br><span class="line">        print(<span class="string">"在第&#123;&#125;步时，k和b更好，此时的Loss是：&#123;&#125;"</span>.format(i, L2))</span><br><span class="line">        </span><br><span class="line">plt.scatter(X_rm, y)</span><br><span class="line">plt.plot(X_rm, price(X_rm, best_k, best_b), color=<span class="string">'red'</span>)</span><br></pre></td></tr></table></figure><pre><code>在第0步时，k和b更好，此时的Loss是：250189.90831387945在第2步时，k和b更好，此时的Loss是：31492.41583403755在第5步时，k和b更好，此时的Loss是：85.14510595256915[&lt;matplotlib.lines.Line2D at 0x7fc010f60f10&gt;]</code></pre><p><img src="https://cdn.jsdelivr.net/gh/rgwang/CDN@latest/2020/08/04/fa3d4829ab099075c9645fb01b9ce957.png" alt="output_17_2"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss_i, loss_y = [i <span class="keyword">for</span> i, l_ <span class="keyword">in</span> loss_update], [l_ <span class="keyword">for</span> i, l_ <span class="keyword">in</span> loss_update]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plt.plot(loss_i, loss_y)</span><br></pre></td></tr></table></figure><pre><code>[&lt;matplotlib.lines.Line2D at 0x7fc010e8db90&gt;]</code></pre><p><img src="https://cdn.jsdelivr.net/gh/rgwang/CDN@latest/2020/08/04/fe704e3344bb5862ca1f6c815e69e3e0.png" alt="output_19_1"></p><h2 id="如何让Loss更快地下降？"><a href="#如何让Loss更快地下降？" class="headerlink" title="如何让Loss更快地下降？"></a><strong>如何让Loss更快地下降？</strong></h2><p>梯度方向是函数增长最快的方向，则负梯度方向是函数下降最快的方向。通过计算函数梯度，在负梯度方向更新自变量的值，就能逐渐减小Loss值。——梯度下降法</p><script type="math/tex; mode=display">L2\_loss=\frac{1}{n}\sum_{i=1}^n(y_{true_i}-\hat y_i)^2</script><script type="math/tex; mode=display">=\frac{1}{n}\sum_{i=1}^n(y_{true_i}-(k\times x_i+b))^2</script><script type="math/tex; mode=display">\frac{\partial loss}{\partial k}=-\frac{2}{n}\sum (y_{true_i}-(k\times x_i+b))x_i</script><script type="math/tex; mode=display">=-\frac{2}{n}\sum (y_{true_i}-\hat y_i)x_i</script><script type="math/tex; mode=display">\frac{\partial loss}{\partial b}=-\frac{2}{n}\sum (y_{true_i}-\hat y_i)</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">partial_k</span><span class="params">(y, yhat, x)</span>:</span> <span class="comment"># loss对k的偏导</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">-2</span> * np.mean((np.array(y)-np.array(yhat)) * np.array(x))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">partial_b</span><span class="params">(y, yhat)</span>:</span> <span class="comment"># loss对b的偏导</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">-2</span> * np.mean((np.array(y)-np.array(yhat)))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">trying_time = <span class="number">1000</span></span><br><span class="line">min_loss = float(<span class="string">'inf'</span>)</span><br><span class="line">best_k, best_b = <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">loss_update = []</span><br><span class="line"></span><br><span class="line">k = random.randint(<span class="number">-200</span>,<span class="number">200</span>)</span><br><span class="line">b = random.randint(<span class="number">-200</span>,<span class="number">200</span>)</span><br><span class="line"></span><br><span class="line">learning_rate = <span class="number">1e-3</span></span><br><span class="line"><span class="comment">#############################梯度下降法求拟合模型#######################</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(trying_time):</span><br><span class="line">    yhat = price(X_rm, k, b)</span><br><span class="line">    L2 = l2_loss(y, yhat)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> L2 &lt; min_loss:</span><br><span class="line">        min_loss = L2</span><br><span class="line">        best_k, best_b = k, b</span><br><span class="line">        loss_update.append([i, L2])</span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">100</span> ==<span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"在第&#123;&#125;步时，k和b更好，此时的Loss是：&#123;&#125;"</span>.format(i, L2))</span><br><span class="line">        </span><br><span class="line">    gradient_k = partial_k(y, yhat, X_rm)</span><br><span class="line">    gradient_b = partial_b(y, yhat)</span><br><span class="line">    </span><br><span class="line">    k = k - gradient_k * learning_rate</span><br><span class="line">    b = b - gradient_b * learning_rate</span><br><span class="line">    </span><br><span class="line">plt.scatter(X_rm, y)</span><br><span class="line">plt.plot(X_rm, price(X_rm, best_k, best_b),color=<span class="string">'red'</span>)</span><br></pre></td></tr></table></figure><pre><code>在第0步时，k和b更好，此时的Loss是：683883.8072032174在第100步时，k和b更好，此时的Loss是：43.75703967614612在第200步时，k和b更好，此时的Loss是：43.73083829219306在第300步时，k和b更好，此时的Loss是：43.730213182251454在第400步时，k和b更好，此时的Loss是：43.72959107251158在第500步时，k和b更好，此时的Loss是：43.728971947626825在第600步时，k和b更好，此时的Loss是：43.728355793276016在第700步时，k和b更好，此时的Loss是：43.72774259520664在第800步时，k和b更好，此时的Loss是：43.72713233923459在第900步时，k和b更好，此时的Loss是：43.72652501124383[&lt;matplotlib.lines.Line2D at 0x7fc010f57e50&gt;]</code></pre><p><img src="https://cdn.jsdelivr.net/gh/rgwang/CDN@latest/2020/08/04/c2b5fb4350db8a5027c1888e8e4b6a49.png" alt="output_24_2"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x_i, x_l = [i <span class="keyword">for</span> i, l <span class="keyword">in</span> loss_update], [l <span class="keyword">for</span> i, l <span class="keyword">in</span> loss_update]</span><br><span class="line">plt.plot(x_i, x_l)</span><br></pre></td></tr></table></figure><pre><code>[&lt;matplotlib.lines.Line2D at 0x7fc010d87c10&gt;]</code></pre><p><img src="https://cdn.jsdelivr.net/gh/rgwang/CDN@latest/2020/08/04/789498565a8534739b250ed87a024518.png" alt="output_25_1"></p><p><strong>选择L1_loss作为评价标准时</strong></p><script type="math/tex; mode=display">L1\_loss=\frac{1}{n}\sum_{i=1}^n|y_{true_i}-\hat y_i|</script><script type="math/tex; mode=display">=\frac{1}{n}\sum_{i=1}^n|y_{true_i}-(k\times x_i+b)|</script><script type="math/tex; mode=display">\frac{\partial loss}{\partial k}=-\frac{1}{n}\sum_{i}^n x_i+\frac{1}{n}\sum_j^n x_j,y_{true_i}-\hat y_i>0, y_{true_j}-\hat y_j<0</script><script type="math/tex; mode=display">\frac{\partial loss}{\partial b}=-\frac{1}{n}\sum_{i}^n 1+\frac{1}{n}\sum_j^n 1,y_{true_i}-\hat y_i>0, y_{true_j}-\hat y_j<0</script><h2 id="选用L1-loss时的梯度下降过程"><a href="#选用L1-loss时的梯度下降过程" class="headerlink" title="选用L1_loss时的梯度下降过程"></a>选用L1_loss时的梯度下降过程</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">l1_partial_k</span><span class="params">(x, y, yhat)</span>:</span></span><br><span class="line">    out = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(x)):</span><br><span class="line">        <span class="keyword">if</span> y[i] &gt; yhat[i]: </span><br><span class="line">            out.append(-x[i])</span><br><span class="line">        <span class="keyword">elif</span> abs(y[i] - yhat[i])&lt;<span class="number">1e-5</span>:</span><br><span class="line">            out.append(<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            out.append(x[i])</span><br><span class="line">            </span><br><span class="line">    <span class="keyword">return</span> np.mean(np.array(out))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">l1_partial_b</span><span class="params">(x, y, yhat)</span>:</span></span><br><span class="line">    out = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(x)):</span><br><span class="line">        <span class="keyword">if</span> y[i] &gt;= yhat[i]:</span><br><span class="line">            out.append(<span class="number">-1</span>)</span><br><span class="line">        <span class="keyword">elif</span> abs(y[i] - yhat[i])&lt;<span class="number">1e-5</span>:</span><br><span class="line">            out.append(<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            out.append(<span class="number">1</span>)</span><br><span class="line">            </span><br><span class="line">    <span class="keyword">return</span> np.mean(np.array(out))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">l1_loss</span><span class="params">(y, yhat)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> np.mean(abs(np.array(y)-np.array(yhat)))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">trying_time = <span class="number">10000</span></span><br><span class="line">min_loss = float(<span class="string">'inf'</span>)</span><br><span class="line">best_k, best_b = <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">loss_update = []</span><br><span class="line"></span><br><span class="line">k = random.randint(<span class="number">-100</span>,<span class="number">100</span>)</span><br><span class="line">b = random.randint(<span class="number">-100</span>,<span class="number">100</span>)</span><br><span class="line"></span><br><span class="line">learning_rate = <span class="number">1e-3</span></span><br><span class="line"><span class="comment">###################L1_loss下的梯度下降过程##################</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(trying_time):</span><br><span class="line">    yhat = price(X_rm, k, b)</span><br><span class="line">    L1 = l1_loss(y, yhat)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> L1 &lt; min_loss:</span><br><span class="line">        min_loss = L1</span><br><span class="line">        best_k, best_b = k, b</span><br><span class="line">        loss_update.append([i, L1])</span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">100</span> ==<span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"在第&#123;&#125;步时，k和b更好，此时的Loss是：&#123;&#125;"</span>.format(i, L1))</span><br><span class="line">        </span><br><span class="line">    gradient_k = l1_partial_k(X_rm, y, yhat)</span><br><span class="line">    gradient_b = l1_partial_b(X_rm, y, yhat)</span><br><span class="line">    </span><br><span class="line">    k = k - gradient_k * learning_rate</span><br><span class="line">    b = b - gradient_b * learning_rate</span><br><span class="line">    </span><br><span class="line">plt.scatter(X_rm, y)</span><br><span class="line">plt.plot(X_rm, price(X_rm, best_k, best_b),color=<span class="string">'red'</span>)</span><br></pre></td></tr></table></figure><pre><code>在第0步时，k和b更好，此时的Loss是：514.2228260869565在第100步时，k和b更好，此时的Loss是：510.17316314868566在第200步时，k和b更好，此时的Loss是：506.1235002104149在第300步时，k和b更好，此时的Loss是：502.07383727214403在第400步时，k和b更好，此时的Loss是：498.0241743338732在第500步时，k和b更好，此时的Loss是：493.9745113956023在第600步时，k和b更好，此时的Loss是：489.92484845733156在第700步时，k和b更好，此时的Loss是：485.8751855190608在第800步时，k和b更好，此时的Loss是：481.8255225807899在第900步时，k和b更好，此时的Loss是：477.77585964251904在第1000步时，k和b更好，此时的Loss是：473.72619670424825在第1100步时，k和b更好，此时的Loss是：469.67653376597747在第1200步时，k和b更好，此时的Loss是：465.62687082770657在第1300步时，k和b更好，此时的Loss是：461.5772078894358在第1400步时，k和b更好，此时的Loss是：457.52754495116494在第1500步时，k和b更好，此时的Loss是：453.4778820128941在第1600步时，k和b更好，此时的Loss是：449.4282190746233在第1700步时，k和b更好，此时的Loss是：445.3785561363524在第1800步时，k和b更好，此时的Loss是：441.32889319808163在第1900步时，k和b更好，此时的Loss是：437.27923025981084在第2000步时，k和b更好，此时的Loss是：433.22956732154在第2100步时，k和b更好，此时的Loss是：429.17990438326916在第2200步时，k和b更好，此时的Loss是：425.13024144499826在第2300步时，k和b更好，此时的Loss是：421.0805785067275在第2400步时，k和b更好，此时的Loss是：417.0309155684567在第2500步时，k和b更好，此时的Loss是：412.98125263018585在第2600步时，k和b更好，此时的Loss是：408.931589691915在第2700步时，k和b更好，此时的Loss是：404.8819267536443在第2800步时，k和b更好，此时的Loss是：400.8322638153734在第2900步时，k和b更好，此时的Loss是：396.78260087710254在第3000步时，k和b更好，此时的Loss是：392.7329379388317在第3100步时，k和b更好，此时的Loss是：388.6832750005609在第3200步时，k和b更好，此时的Loss是：384.63361206229007在第3300步时，k和b更好，此时的Loss是：380.5839491240192在第3400步时，k和b更好，此时的Loss是：376.53428618574844在第3500步时，k和b更好，此时的Loss是：372.48462324747754在第3600步时，k和b更好，此时的Loss是：368.4349603092068在第3700步时，k和b更好，此时的Loss是：364.3852973709359在第3800步时，k和b更好，此时的Loss是：360.3356344326651在第3900步时，k和b更好，此时的Loss是：356.2859714943943在第4000步时，k和b更好，此时的Loss是：352.23630855612345在第4100步时，k和b更好，此时的Loss是：348.18664561785266在第4200步时，k和b更好，此时的Loss是：344.13698267958176在第4300步时，k和b更好，此时的Loss是：340.087319741311在第4400步时，k和b更好，此时的Loss是：336.0376568030401在第4500步时，k和b更好，此时的Loss是：331.98799386476935在第4600步时，k和b更好，此时的Loss是：327.93833092649857在第4700步时，k和b更好，此时的Loss是：323.8886679882277在第4800步时，k和b更好，此时的Loss是：319.839005049958在第4900步时，k和b更好，此时的Loss是：315.7893421116916在第5000步时，k和b更好，此时的Loss是：311.7396791734253在第5100步时，k和b更好，此时的Loss是：307.69001623515896在第5200步时，k和b更好，此时的Loss是：303.6403532968926在第5300步时，k和b更好，此时的Loss是：299.5906903586262在第5400步时，k和b更好，此时的Loss是：295.54102742035985在第5500步时，k和b更好，此时的Loss是：291.4913644820935在第5600步时，k和b更好，此时的Loss是：287.4417015438272在第5700步时，k和b更好，此时的Loss是：283.3920386055608在第5800步时，k和b更好，此时的Loss是：279.34237566729445在第5900步时，k和b更好，此时的Loss是：275.29271272902804在第6000步时，k和b更好，此时的Loss是：271.24304979076175在第6100步时，k和b更好，此时的Loss是：267.19338685249534在第6200步时，k和b更好，此时的Loss是：263.143723914229在第6300步时，k和b更好，此时的Loss是：259.09406097596263在第6400步时，k和b更好，此时的Loss是：255.04439803769628在第6500步时，k和b更好，此时的Loss是：250.99473509942993在第6600步时，k和b更好，此时的Loss是：246.94507216116358在第6700步时，k和b更好，此时的Loss是：242.89540922289717在第6800步时，k和b更好，此时的Loss是：238.84574628463085在第6900步时，k和b更好，此时的Loss是：234.79608334636447在第7000步时，k和b更好，此时的Loss是：230.7464204080981在第7100步时，k和b更好，此时的Loss是：226.69675746983174在第7200步时，k和b更好，此时的Loss是：222.6470945315654在第7300步时，k和b更好，此时的Loss是：218.597431593299在第7400步时，k和b更好，此时的Loss是：214.54776865503268在第7500步时，k和b更好，此时的Loss是：210.4981057167663在第7600步时，k和b更好，此时的Loss是：206.44844277849992在第7700步时，k和b更好，此时的Loss是：202.3987798402336在第7800步时，k和b更好，此时的Loss是：198.34911690196722在第7900步时，k和b更好，此时的Loss是：194.29945396370084在第8000步时，k和b更好，此时的Loss是：190.2497910254345在第8100步时，k和b更好，此时的Loss是：186.20012808716814在第8200步时，k和b更好，此时的Loss是：182.15046514890176在第8300步时，k和b更好，此时的Loss是：178.1008022106354在第8400步时，k和b更好，此时的Loss是：174.05113927236906在第8500步时，k和b更好，此时的Loss是：170.0014763341027在第8600步时，k和b更好，此时的Loss是：165.95181339583633在第8700步时，k和b更好，此时的Loss是：161.90215045756997在第8800步时，k和b更好，此时的Loss是：157.85248751930362在第8900步时，k和b更好，此时的Loss是：153.80282458103724在第9000步时，k和b更好，此时的Loss是：149.7531616427709在第9100步时，k和b更好，此时的Loss是：145.70349870450454在第9200步时，k和b更好，此时的Loss是：141.6538357662382在第9300步时，k和b更好，此时的Loss是：137.6041728279718在第9400步时，k和b更好，此时的Loss是：133.55450988970543在第9500步时，k和b更好，此时的Loss是：129.50484695143908在第9600步时，k和b更好，此时的Loss是：125.45518401317273在第9700步时，k和b更好，此时的Loss是：121.40552107490637在第9800步时，k和b更好，此时的Loss是：117.35585813664001在第9900步时，k和b更好，此时的Loss是：113.30619519837286[&lt;matplotlib.lines.Line2D at 0x7fc010f27490&gt;]</code></pre><p><img src="https://cdn.jsdelivr.net/gh/rgwang/CDN@latest/2020/08/04/98eee24a6c9ca30e799063656f67c3b4.png" alt="output_29_2"></p>]]></content>
      
      
      <categories>
          
          <category> Deep Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DL </tag>
            
            <tag> Gradient Descent </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>YOLOv1(You Only Look Once)</title>
      <link href="/2020/07/13/YOLOv1(You-Only-Look-Once)/"/>
      <url>/2020/07/13/YOLOv1(You-Only-Look-Once)/</url>
      
        <content type="html"><![CDATA[<h1 id="REFENRECE"><a href="#REFENRECE" class="headerlink" title="REFENRECE"></a>REFENRECE</h1><p>1.<a href="https://mp.weixin.qq.com/s?__biz=MzI5MDUyMDIxNA==&amp;mid=2247494712&amp;idx=3&amp;sn=fe711048161e9c4d11b95e887fe041a0&amp;chksm=ec1c01c1db6b88d7c25a6a812acdf94e944542a1db9abe6c42bf3f5e202abe02f6b562c1a719&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">https://mp.weixin.qq.com/s?__biz=MzI5MDUyMDIxNA==&amp;mid=2247494712&amp;idx=3&amp;sn=fe711048161e9c4d11b95e887fe041a0&amp;chksm=ec1c01c1db6b88d7c25a6a812acdf94e944542a1db9abe6c42bf3f5e202abe02f6b562c1a719&amp;scene=21#wechat_redirect</a></p><p>2.<a href="https://www.jianshu.com/p/cad68ca85e27" target="_blank" rel="noopener">https://www.jianshu.com/p/cad68ca85e27</a></p><p>3.<a href="https://blog.csdn.net/u014380165/article/details/72616238?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-4&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-4" target="_blank" rel="noopener">https://blog.csdn.net/u014380165/article/details/72616238?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-4&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-4</a></p><h1 id="YOLOv1"><a href="#YOLOv1" class="headerlink" title="YOLOv1"></a>YOLOv1</h1><p>　　faster-RCNN之后，rbg(RossGirshick)提出的另一种目标检测框架YOLO。</p><p>　　论文下载：<a href="http://arxiv.org/abs/1506.02640" target="_blank" rel="noopener">http://arxiv.org/abs/1506.02640</a></p><p>　　代码下载：<a href="http://github.com/pjreddie/darknet" target="_blank" rel="noopener">http://github.com/pjreddie/darknet</a></p><h2 id="1-YOLO的核心思想"><a href="#1-YOLO的核心思想" class="headerlink" title="1.YOLO的核心思想"></a>1.YOLO的核心思想</h2><p>　　利用整张图作为网络输入，直接在输出层回归bbox的位置和所属类别。YOLOv1在速度上有大幅提升，处理速度可达到45fps，其快速版本（网络较小）甚至可以达到155fps。</p><h2 id="2-YOLO的实现方法"><a href="#2-YOLO的实现方法" class="headerlink" title="2.YOLO的实现方法"></a>2.YOLO的实现方法</h2><p>　　将一幅图像分成$S\times S$个网格，若某个object中心落在这个网格中，则这个网格就负责预测这个object。</p><p><img src="https://cdn.jsdelivr.net/gh/rgwang/CDN@latest/2020/07/11/5a713128a524820e79255f09926b5209.png" alt=""></p><p>　　每个网格要预测2个bbox，每个bbox除了要回归自身坐标外，还要附带预测一个confidence值。每个bbox要预测(x,y,w,h)和confidence共5个值，每个网格要预测类别信息，记为C。每个网格还要预测2个bbox。则整幅图的输出就是$S\times S\times (2\times 5+C)$。</p><p><img src="https://cdn.jsdelivr.net/gh/rgwang/CDN@latest/2020/07/12/ad14e3f255a002e8a6bcdbcc136321a8.png" style="zoom:80%;" /></p><ol><li><p>C指的是类别数，每个输出tensor里有C个位置记录该网格存在某一种目标的概率，可记为</p><p>　　　　　　　　　　　　<script type="math/tex">P(C_{1}|Object),\cdots ,P(C_{i}|Object),\cdots</script></p><p>理解成条件概率（当网格存在object时，该object是$C_{i}$类的概率）。</p></li><li><p>每个bbox需要4个数值来表示位置，(Center_x,Center_y,width,height)。</p></li><li><p>bbox的置信度confidence</p><p>　　　　　　　　　　　　$confidence=Pr(Object)\times IOU_{pred}^{truth}$</p><p>这个confidence代表所预测的bbox中含有object的概率和该box预测有多准两重信息。其中若有object落在一个网格中，则$Pr(Object)=1$，否则取0；第二项指预测的bbox和实际groundtruth之间的IoU值。</p></li></ol><h2 id="3-讨论"><a href="#3-讨论" class="headerlink" title="3.讨论"></a>3.讨论</h2><ol><li><p>类别信息是针对每个网格的，confidence是针对每个bbox的。</p></li><li><p>　　YOLOv1的bbox并不是faster RCNN的Anchor。faster RCNN采用手工设置好的anchor，每个anchor有不同的大小和宽高比。YOLOv1并没有预先设置bbox的大小和形状，这里的bbox更像是进化算法，即事先并不知道会在什么位置，需经过前向计算，网络输出2个bbox。训练开始阶段，网络预测的bbox可能都是乱来的，但总是选择IoU相对大一些的那个bbox继续训练，每个bbox会逐渐擅长对某些情况的预测。</p></li><li><p>训练样本构造</p><p><img src="https://cdn.jsdelivr.net/gh/rgwang/CDN@latest/2020/07/12/f54950a926674fc20e4b650e39bb79b2.png" style="zoom:67%;" /></p><p><strong>①</strong>　　对于输入图像中的每个对象，先找到中心点，如上图中的自行车，中心点在黄色圆点位置，则该黄色网格对应的标签中，自行车的概率设为1，其他对象的概率设为0。所有其他48个网格的标签中，该自行车的概率都设为0。（这就是中心点所在的网格对预测该对象负责）</p><p><strong>②</strong>　　每个网格的输出包含2个bbox，每个bbox又包含一个confidence值。比较2个bbox的IoU，IoU大的那个bbox的$Pr(Object)=1$，同时真实bbox的值也就填入标签对应的bbox。另一个不负责预测的bbox的$Pr(Object)=0$。</p><p><img src="https://cdn.jsdelivr.net/gh/rgwang/CDN@latest/2020/07/12/fab11a59ae9f6579517b7d631d951acf.png" style="zoom:50%;" /></p><p><strong>注</strong>：上图将自行车的真实位置放在bbox1，但实际是在训练过程中等网络输出以后，比较两个bbox与自行车真实位置的IoU，自行车的真实位置放在IoU比较大的那个bbox中，且将该bbox的confidence置为1。</p></li><li><p>损失函数</p><p><img src="https://cdn.jsdelivr.net/gh/rgwang/CDN@latest/2020/07/12/fbac459f013bd6d6085e3de8d7738553.png" style="zoom:50%;" /></p><p>损失函数如下：</p><p><img src="https://cdn.jsdelivr.net/gh/rgwang/CDN@latest/2020/07/12/529cf3f362320cb27b228e1062fb1d81.png" style="zoom: 80%;" /></p><p><strong>①</strong>目标分类的误差：公式第5行表示存在object的网格才计入误差。</p><p><strong>②</strong>bbox的位置误差：公式第1行和第2行</p><ol><li><p>都有系数$1_{ij}^{obj}$表示只有负责(IoU比较大)预测的那个bbox计入误差。</p></li><li><p>第2行公式中宽和高都先取了平方根，这样做是因为相同的宽和高的误差对于小目标精度影响比大目标要大。比如，原始w=10,h=20，预测w=8,h=22和原始w=3,h=5，预测w=1,h=7相比，其实前者误差比后者小，但如果不开平方根，则损失是相等的：4+4=8，而取平方根后，变成0.15和0.7。</p><p><img src="https://cdn.jsdelivr.net/gh/rgwang/CDN@latest/2020/07/12/8e0f4b6e33129fec4c1fd2e9af602302.png" alt=""></p><p>由上图知，取平方根后，小box的在横轴上的值较小，发生偏移时，反应到y轴上的偏差比大box要大。</p></li></ol><p><strong>③</strong>bbox的confidence误差：</p><ol><li>公式第三行是存在object的bbox的confidence误差。系数$1_{ij}^{obj}$表示只有负责(IoU比较大)预测的那个bbox的confidence才会计入这项误差。</li><li>公式第四行是不存在object的bbox的confidence误差。若该项不恰当地输出较高的confidence，则会与真正负责该object预测的那个bbox混淆。</li></ol><p><strong>④</strong>几个问题</p><ol><li>8维的localization error和20维的classification error同等重要是不合理的。</li><li>若一个网格中没有object（一幅图中这种网格很多），那么就会将这些网格中的bbox的confidence push到0，相比于较少的有object的网格，这种做法会导致网络不稳定甚至发散。</li></ol><p><strong>⑤</strong>解决办法</p><ul><li>更重视8维的坐标预测，给这些损失前面赋予更大的权重$λ_{coord}$，在pascal VOC训练中取5。</li><li>对没有object的bbox的confidence loss，赋予较小的权重$λ_{noobj}$，在pascal VOC训练中取0.5。</li><li>有object的bbox的confidence loss和类别的loss的权重取1。</li></ul></li><li><p>　　在test的时候，每个网格预测的class信息和bbox预测的confidence信息相乘，就得到每个bbox的class-specific confidence score：</p><p>　　　　　　　　　　 　<script type="math/tex">Pr(Class_{i}|Object)\times Pr(Object)\times IOU_{pred}^{truth}=Pr(Class_{i})\times IOU_{pred}^{truth}</script></p><p>　　等式左边第一项是每个网格预测的类别信息，后两项是每个bbox的confidence。该乘积即encode了预测的box属于某一类的概率，也包含该box准确度的信息。</p><p>　　得到每个box的class-specific conficence score以后，设置阈值，滤掉得分低的boxes，对保留的boxes进行NMS处理，得到最终的检测结果。</p></li><li><p>NMS（非极大值抑制）</p><p>　　核心思想：选择得分最高的作为输出，与该输出重叠的去掉，不断重复该过程直到所有备选处理完。</p><p>   　　<strong>具体算法</strong>：设网络输出$7\times 7\times 30$的tensor，在每个网格中，对象$C_{i}$位于第j个bbox的得分为：</p><p>   　　　　　　　　　　　　　　　　<script type="math/tex">score_{ij}=P(C_{i}|Object)\times Confidence_{j}</script></p><p>代表某类对象$C_{i}$存在于第j个bbox的可能性。</p><p>　　每个网格有20个对象(pascal VOC有20类)的概率$\times$2个bbox的confidence，共40个得分。则$7\times 7$个网格共有1960个得分。对每种对象进行NMS，每种对象有1960/20=98个得分。</p><ol><li>设置一个score的阈值，低于该阈值的候选得分排除掉（将score设为0）</li><li>遍历每个对象类别<ol><li>遍历当前对象的98个得分</li><li>找到score最大的那个bbox，添加到输出列表</li><li>对每个score不为0的候选对象，计算其与上面输出对象的bbox的IoU</li><li>根据预先设置的IoU阈值，所有高于该阈值（重叠度较高）的候选对象排除掉（将score设为0）</li><li>如果所有bbox要么在输出列表中，要么score=0，则该对象类别的NMS完成，返回步骤b处理下一种对象</li></ol></li><li>输出列表即为预测的对象</li></ol></li><li><p>激活函数使用leak RELU。</p></li><li><p>　　输出层为全连接层，因此在检测时，模型只支持与训练图像相同的输入分辨率。（因为全连接层神经元固定，接受的输入大小也就固定。而卷积网络因为使用卷积核处理数据，无论输入大小如何，卷积核可始终保持不变。）</p></li><li><p><strong>缺点</strong></p><ol><li>YOLOv1对相互靠得很近的物体，以及很小的物体检测效果不好，因为一个网格只预测了两个框，且都指向一类物体。此外，Pooling层会丢失一些信息，对定位存在影响。</li><li>同一类物体出现的新的不常见的长宽比和其他情况时，泛化能力较弱。</li><li>由于损失函数的问题，定位误差是影响检测效果的主要原因。尤其是大小物体的处理上，还有待加强。</li></ol></li></ol>]]></content>
      
      
      <categories>
          
          <category> object detection </category>
          
          <category> YOLO </category>
          
      </categories>
      
      
        <tags>
            
            <tag> object deteciton </tag>
            
            <tag> YOLOv1 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>如何阅读期刊论文</title>
      <link href="/2020/06/12/%E5%A6%82%E4%BD%95%E9%98%85%E8%AF%BB%E6%9C%9F%E5%88%8A%E8%AE%BA%E6%96%87/"/>
      <url>/2020/06/12/%E5%A6%82%E4%BD%95%E9%98%85%E8%AF%BB%E6%9C%9F%E5%88%8A%E8%AE%BA%E6%96%87/</url>
      
        <content type="html"><![CDATA[<p>​         技术的创新并不是全靠聪明。只要<strong>学会分析期刊论文的优缺点</strong>，就可拿这套方法分析竞争对手产品的优缺点；而且，只要再稍微加工，就可以从这套优缺点的清单里找到突破瓶颈所需的关键性创意。这套创新程序，可以把「创新」变成不需要太多天分便可以完成的事，从而减轻创意的不定性与风险性。因此，只要会分析论文，几乎就可以轻易地组合出你所需要的绝大部分创意。聪明是不可能教的，但这套技巧却是可以教的；而且只要用心，绝大部分硕士生都可以学会。</p><h1 id="期刊论文的分析技巧与程序"><a href="#期刊论文的分析技巧与程序" class="headerlink" title="期刊论文的分析技巧与程序"></a>期刊论文的分析技巧与程序</h1><p>一篇期刊论文，主要分成四个部分。</p><h2 id="1-Abstract"><a href="#1-Abstract" class="headerlink" title="1. Abstract"></a>1. Abstract</h2><p>​         涉及这篇论文的主要贡献、方法特色与主要内容。须学会只看Abstract和Introduction就判断出这篇论文的重点和自己的研究是否有直接关联，从而决定要不要把它给读完。</p><h2 id="2-Introduction"><a href="#2-Introduction" class="headerlink" title="2. Introduction"></a>2. Introduction</h2><p>​         介绍问题的背景和起源，交代前人在这个题目上已经有过的主要贡献，说清楚前人留下来的未解问题，以及在这个背景下这篇论文想解决的问题和它的重要性。</p><p>​         对初学者而言，先收集与课题相关的论文30~40篇，每篇都只读Abstract和Introduction，不读Main Body，只在必要时稍微参考一下文后的Illustrative examples和Conclusions，直到能回答以下三个问题：</p><p>（2A）在该领域内最常被引述的方法有哪些？</p><p>（2B）这些方法可以分成哪些主要派别？</p><p>（2C）每个派别的主要特色（含优点和缺点）是什么？</p><p>​        如何找到这30~40篇论文？有一种期刊论文叫review paper，在keywords中加一个review筛选出这类论文，从相关的数篇review paper开始，从中根据title和Abstract找出和自己研究课题相关的30~40篇论文。</p><p>​         通常反复读过这30~40篇论文的Abstract和Introduction，就可以回答（2A）和（2B）。要回答（2A）和（2B），应先挑那些Introduction写的比较有观念的论文（不要直接读写得像流水账的Introduction）。</p><p>​        假如读过30~40篇论文的Abstract和Introduction后，还是回答不了（2C），就先做以下工作：</p><ul><li>先根据（2A）的答案，把该领域内最常被引述的论文找齐，再把他们根据（2B）的答案分成派别，每个派别按日期先后次序排好。然后，每次只重新读一派的Abstract和Introduction（必要时简略参考内文，但目的只是读懂Introduction内与这派有关的陈述，而不需要真的看懂所有内文），照日期先后读，读的时候只考虑回答一个问题：这一派的创意和主要诉求是什么？这样把每一派的Abstract和Introduction读完，总结出这一派主要的诉求、方法特色和优点。</li><li>其次，重读前面这些论文的Introduction，回答问题：每篇论文对其他派别有什么批评？然后把读到的重点逐一记录到各派别的<strong>缺点</strong>栏内。</li></ul><p>​        通过以上程序，可以掌握到（2A）、（2B）和（2C）的答案。这时应该对该领域内主要方法、文献之间的关系比较熟悉了。此时，可以用这些论文测试看看之前用来搜寻该领域论文的keywords恰不恰当，再用修正过的keywords再搜寻一次论文，把该领域的主要文献补齐，也把原来30~40篇论文中关系较远的论文删除，只保留大概20篇左右确定跟自己关系较近的文献。甚至可以删除几个不想用的派别（要有充分理由），只保留另几个派别（也要有充分理由）。</p><p>​         然后再利用（2C）的答案，再进一步回答一个问题（2D）：<strong>这个领域内大家认为重要的关键问题有哪些？有哪些特性是大家重视的优点？有哪些特性是大家在意的缺点？这些优点与缺点通常在哪些应用场合时会比较被重视？在哪些应用场合时比较不会被重视？</strong>然后就可以整理出该领域主要的应用场合，以及这些应用场合上该注意的事项。</p><p>​         最后，根据（2A）和（2C）的答案，把各派别内的论文整理在同一个档案内，按时间顺序排好，然后依照这些派别与自己研究方向的关系远近，逐一把各派论文的Main Body读完。</p><h2 id="3-Main-Body（simulation-and-experimental-examples"><a href="#3-Main-Body（simulation-and-experimental-examples" class="headerlink" title="3. Main Body（simulation and experimental examples)"></a>3. Main Body（simulation and experimental examples)</h2><p>第一次有系统地读某派别的论文Main Body时，只需要读懂：</p><p>（3A）该论文的主要假设是什么（什么条件下是有效的），并评估下这些假设在现实条件下成立的难度。越难成立的假设，越不好用，参考价值也越低。</p><p>（3B）在这些假设下，这篇论文主要有什么好处。</p><p>（3C）这些好处主要表现在哪些公式的哪些项目的简化上。不需要懂这篇论文详细的推导过程。除了三、五个关键的公式（最后在应用上要使用的公式，可以从这些公式评估出该方法使用上的方便程度或计算效率，以及在非理想情境下这些公式使用起来的可靠度或稳定性），其他公式弄不懂也没事，公式之间的恒等式推导过程可完全略过。假如要看公式，重点应看公式推导过程中引入的假设条件，而不是恒等式的推导。</p><p>​         但是，在开始根据前述问题读论文前，应先把收集的该派别所有论文都拿出来，逐篇粗略浏览过去（不要勉强自己每篇或每行都弄懂，而是轻松读，能懂就懂，不懂就不懂），从中挑出容易读懂的papers，以及经常被引述的论文。然后把这些论文按时间顺序依次读下去。读的时候，记得只回答（3A）、（3B）、（3C）就好，不用读太细致。</p><p>​        这样读完论文后，应该把这一派的主要发展过程、主要假设、主要理论依据及主要成果做一个完整梳理。其次，还要根据（2D）的答案及这一派的主要假设，进一步回答问题：（3D）这一派主要的缺点有哪些。最后，根据（3A）、（3B）、（3C）、（3D）的答案综合整理出：这一派最适合什么时候使用，最不适合什么场合使用。</p><p>​        论文作者常常故意只提成功的实验案例，所以simulation examples and experiments表现好不代表这个方法真的很好。必须回到这个方法的基本假设以及在用该方法时所使用的主要公式（resultant equations)上去，参考（2C）和（2D）的答案，问自己：当某个假设无法成立时，该方法会不会出什么状况？猜测该方法应该会在哪些应用场合表现优异，又会在哪些应用场合出状况？根据猜测再检验一次simulation examples and experiments，看其优点和缺点是否确实在这些examples中被充分检验且充分表现出来。</p><p>==注==：任何时候都不需要弄懂一篇论文所有的恒等式推导过程，不需要把整篇论文细细读完，只需要把确定会用到的部分完全弄懂就好，其他的也只需要了解它主要的idea。</p><p><img src="http://images.cnitblog.com/i/326116/201403/211603473659485.png" alt="img"></p><h1 id="方法与应用场合特性表（有迹可寻的创意产生程序）"><a href="#方法与应用场合特性表（有迹可寻的创意产生程序）" class="headerlink" title="方法与应用场合特性表（有迹可寻的创意产生程序）"></a>方法与应用场合特性表（有迹可寻的创意产生程序）</h1><p>从上图的步骤（4）和（5）获得以下两张表：</p><p><img src="http://images.cnitblog.com/i/326116/201403/211602235683420.png" alt="img"></p><p>​         同样一个方法可能有许多不同的应用场合，而不同应用场合可能会对适用（或最佳）的方法有不同要求。<strong>方法没有好坏，只有相对优缺点；只有当方法的特性与应用场合的特性不合时，才能下结论说这方法「不适用」；而当方法的特性与应用场合的特性吻合时，则下结论说这方法「很适用」。</strong></p><p>==技巧==：上面的方法与问题分析对照表还可以用来把「突破瓶颈所需的创意」简化成一种「有迹可寻」的工作。譬如，假定我们要针对应用甲发展一套适用的方法，首先我们要先从上右表中标定这个应用场合关心哪些问题特性。根据上右表第一个 column，甲应用场合只关心四个特性：特性1、2、3、5。哪个方法最适用呢？看起来是方法一，它除了特性2表现普通之外，其它三个特性的表现都很出色。但是，假如我们对方法一的表现仍不够满意，怎么去改善它？最简单的办法就是从上左表找现成的方法和方法一结合，产生出一个更适用的方法。因为方法一只有在特性2上面表现不够令人满意，所以我们就优先针对在特性2上面表现出色的其它方法加以研究。根据上左表，在特性2上面表现出色的方法有方法二和方法四，所以我们就去研究这两个方法和方法一结合的可能性。或许（随便举例）方法四的创意刚好可以被结合进方法一而改善方法一在特性2上面的表现，那么，我们就可以因此轻易地获得一个方法一的改良，从而突破甲应用场合没有适用方法的瓶颈。</p><p><strong>多半时候只要应用上一段的分析技巧就可以产生足以解决实用问题的创意了。</strong></p><h1 id="论文阅读的补充说明"><a href="#论文阅读的补充说明" class="headerlink" title="论文阅读的补充说明"></a>论文阅读的补充说明</h1><p>不好的习惯：</p><p>（1）老是想逐行读懂，有一行读不懂就受不了。</p><p>（2）不敢发挥自己的想象，读论文像在读教科书，论文没写的就不会，瘫痪在那里；自己猜测或想象时，老怕弄错作者的意思，神经绷紧，脑筋根本动不了。</p><p>==注==：每次读论文都一定要带着问题去读，每次读的时候都只是图回答你要回答的问题。因此，一定是选择性地阅读，一定要逐渐由粗而细地一层一层去了解。一定是一整批一起读懂到某个层次，而不是逐篇逐篇地整篇一次读懂。</p><p>​        许多论文中没被交代的段落你也已经可以有一些属于你的想象，猜完以后要根据你的猜测在论文里找证据，用以判断你的猜测对不对。猜对了，就用你的猜测（其实是你的推理架构）去吸收作者的资讯与创意；猜错了，论文理会有一些信息告诉你说你错了，而且因为猜错所以你读到对的答案时反而印象更深刻。</p><h1 id="论文报告的要求与技巧"><a href="#论文报告的要求与技巧" class="headerlink" title="论文报告的要求与技巧"></a>论文报告的要求与技巧</h1><p>报告一篇论文（依报告次序排列）：</p><p>　　（1） 投影片第一页必须列出论文的题目、作者、论文出处与年份。</p><p>　　（2） 以下每一页投影片只能讲一个观念，不可以在一张投影片里讲两个观念。</p><p>　　（3） 说明这篇论文所研究的问题的重点，以及这个问题可能和工业界的哪些应用相关。</p><p>　　（4） 清楚交代这篇论文的主要假设，主要公式，与主要应用方式（以及应用上可能的解题流程）。</p><p>　　（5） 说明这篇论文的范例（simulation examples and/or experiments），预测这个方法在不同场合时可能会有的准确度或好用的程度</p><p>　　（6） 你个人的分析、评价与批评，包括：</p><p>（6A）这篇论文最主要的创意是什么？</p><p>（6B）这些创意在应用上有什么好处？</p><p>（6C）这些创意和应用上的好处是在哪些条件下才能成立？</p><p>（6D）这篇论文最主要的缺点或局限是什么？</p><p>（6E）这些缺点或局限在应用上有什么坏处？</p><p>（6F）这些缺点和应用上的坏处是因为哪些因素而引入的？</p><p>（6G）你建议学长学弟什么时候参考这篇论文的哪些部分（点子）？</p><p>　　一般来讲，刚开始报告论文（硕一上学期）时只要做到能把前四项要素说清楚就好了，但是硕一结束后（暑假开始）必须要设法做到六项要素都能触及。硕二下学期开始的时候，必须要做到六项都能说清楚。</p><p>　　注意：读论文和报告论文时，最重要的是它的创意和观念架构，而不是数学上恒等式推导过程的细节（顶多只要抓出关键的 equation 去弩懂以及说明清楚即可）。你报告观念与分析创意，别人容易听懂又觉得有趣；你讲恒等式，大家不耐烦又浪费时间。</p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p><a href="https://blog.csdn.net/symoriaty/article/details/76578180?utm_medium=distribute.pc_relevant_right.none-task-blog-OPENSEARCH-4&amp;depth_1-utm_source=distribute.pc_relevant_right.none-task-blog-OPENSEARCH-4" target="_blank" rel="noopener">https://blog.csdn.net/symoriaty/article/details/76578180?utm_medium=distribute.pc_relevant_right.none-task-blog-OPENSEARCH-4&amp;depth_1-utm_source=distribute.pc_relevant_right.none-task-blog-OPENSEARCH-4</a></p>]]></content>
      
      
      <categories>
          
          <category> postgraduate </category>
          
      </categories>
      
      
        <tags>
            
            <tag> papers reading </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>目标检测mAP(mean Average Precision)</title>
      <link href="/2020/06/09/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8BmAP(mean%20Average%20Precision)/"/>
      <url>/2020/06/09/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8BmAP(mean%20Average%20Precision)/</url>
      
        <content type="html"><![CDATA[<p>​         mAP可译为平均精度均值，是目标检测中模型性能的衡量指标。多个类别的目标检测任务中，每个类别都可以根据recall（召回率）和precision（准确率）绘制一条曲线。AP可看作该曲线下的面积，而mAP就是指在求得每一类AP的基础上再计算其所有类别的平均值。</p><h2 id="1-几个概念"><a href="#1-几个概念" class="headerlink" title="1.几个概念"></a>1.几个概念</h2><ol><li><p>True Positives(TP)：实际为正例且被模型划分为正例的实例数。</p></li><li><p>False Positives(FP)：实际为负例但被模型划分为正例的实例数。</p></li><li><p>True  Negatives(TN)：实际为负例且被模型划分为负例的实例数。</p></li><li><p>False Negatives(FN)：实际为正例但被模型划分为负例的实例数。</p></li><li><p>准确率（Precision）可理解为<strong>查准率</strong>，是指在所有预测为正例的样本中，真正例所占的比例。</p><p>召回率 $(\mathrm{recall})=\frac{TP}{TP+FN}=\mathrm{R}$<br>(查全率)</p></li><li><p>召回率（Recall）可理解为<strong>查全率</strong>，是指在所有正例中被正确预测的比例。</p><p>准确率 $(\text { precision })=\frac{TP}{TP+FP}=\mathrm{P}$<br>$($ 查准率)</p><p><img src="https://cdn.jsdelivr.net/gh/rgwang/CDN@latest/2020/06/29/31e94cef78cf781b3f549e250d1ccb36.png" style="zoom:80%;" /></p></li></ol><h2 id="2-单类别AP"><a href="#2-单类别AP" class="headerlink" title="2.单类别AP"></a>2.单类别AP</h2><p>​        目标检测的预测结果通常包含两部分，即预测框(bounding box)和置信度P。而预测正确需要满足两个条件，①类别正确且置信度(confidence score)大于一定阈值(P_threshold)，②预测框与真实框(ground truth)的IoU大于一定阈值(IoU_threshold)。</p><p><strong>示例</strong></p><p>​         假设用训练好的模型得到所有测试样本的confidence score，每一类的confidence score保存到一个文件中，设共有20个测试样本，每个样本的id、confidence score和ground truth label如下。</p><p><img src="https://cdn.jsdelivr.net/gh/rgwang/CDN@latest/2020/06/29/fb7774ce5bf0cb75ee27796c374d09f7.png" style="zoom:80%;" /></p><p>然后对confidence score 排序得到，</p><p><img src="https://cdn.jsdelivr.net/gh/rgwang/CDN@latest/2020/06/29/3572ee3351d6f517d777fabe9c7b35c6.png" style="zoom:80%;" /></p><p>再计算precision和recall。比如想得到top-5的结果，则相当于在设定了置信阈值的情况下，上表中前5个样本被认定为正例，其余均为负例。</p><p><img src="https://cdn.jsdelivr.net/gh/rgwang/CDN@latest/2020/06/29/70605dd4b6c9bedfc82f756d391e7d92.png" style="zoom:80%;" /></p><p>则在这个例子中，True Positives就是指id为4和2的样本，因为它们的gt_label为1且同时被预测为正例。False Positives就是指id为13、19、6的样本。而从全表看，gt_label为1的有6个样本，即False Negatives为4个，True Negatives为15-4=11个。</p><p>因此，对于top-5而言，Precision=2/(2+3)=0.4，Recall=2/(2+4)=1/3。</p><p>在实际多类别检测任务中，通常不会只通过top-5来衡量模型的好坏，而是需要知道从top-1到top-N（N是所有测试样本个数）对应的Precision和Recall。易知，随着参与计算的样本增加，Recall会越来越大，Precision则整体呈下降趋势。把Recall作为横坐标，Precision 作为纵坐标，即可得到Precision-Recall曲线。</p><p><img src="https://cdn.jsdelivr.net/gh/rgwang/CDN@latest/2020/06/29/6cdc453a2895a81d77f2374012808a7b.png" style="zoom:80%;" /></p><p>在计算AP之前，需要先将平滑化。方法是<strong>取查全率大于等于r时最大的查准率p</strong>。即，$p(r)=\max _{\tilde{r} \geq r} p(\tilde{r})$。</p><p>从而得到平滑后的曲线（下图仅为示意图）。</p><p><img src="https://cdn.jsdelivr.net/gh/rgwang/CDN@latest/2020/06/29/6cd16cdf7078d91282559d9d49a9c4fa.png" style="zoom:80%;" /></p><p>而对于AP的计算有两种方法：</p><ol><li><p>voc2010之前的方法</p><p>AP=(平滑后PR曲线上，Recall分别等于0,0.1,0.2,…,1.0等11处Precision的平均值)</p><p>$A P=\frac{1}{11} \sum_{r \subseteq{0,0.1, . ., 1.0}} p(r)$</p></li><li><p>voc2010以后的方法</p><p>AP=平滑后PR曲线下的面积</p></li></ol><h2 id="3-mAP的计算"><a href="#3-mAP的计算" class="headerlink" title="3.mAP的计算"></a>3.mAP的计算</h2><ol><li><p>voc数据集的mAP</p><p>voc数据集中的mAP计算的是IoU_threshold=0.5时各个类别AP的均值。</p></li><li><p>coco数据集的mAP</p><p>coco认为固定IoU_threshold的取值无法有效衡量对模型性能的影响。</p><p>比如A模型在IoU_threshold=0.5时，mAP=0.4，而B模型在IoU_threshold=0.7时，mAP同样为0.4。根据voc的标准，A、B模型的性能一样，但显然B模型的预测框更准确，性能更好。</p><p>故，coco计算IoU_threshold=0.5,0.55,0.6,…,0.95时的各个mAP。</p></li></ol><h2 id="REFERENCE"><a href="#REFERENCE" class="headerlink" title="REFERENCE"></a>REFERENCE</h2><p><a href="https://blog.csdn.net/william_hehe/article/details/80006758" target="_blank" rel="noopener">https://blog.csdn.net/william_hehe/article/details/80006758</a></p><p><a href="https://zhuanlan.zhihu.com/p/56961620" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/56961620</a></p>]]></content>
      
      
      <categories>
          
          <category> object detection </category>
          
      </categories>
      
      
        <tags>
            
            <tag> object detection </tag>
            
            <tag> mAP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2020/06/04/hello-world/"/>
      <url>/2020/06/04/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
